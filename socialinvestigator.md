This project creates and updates a Google Sheet with relevant tweets and a basic sentiment analysis of this tweet.

[Here](https://hub.gke.mybinder.org/user/ethelk-social-investigator-wlb7w3pj/notebooks/Social_Investigator.ipynb) is a full Jupyter Notebook of the project. 

**The problem**

I was monitoring a few companies in the context of a larger project and needed to get an update of their activity and what the public thought about them. While I was reading the news and checking sometimes social media, I needed to find a more efficient way to see and analysis what was going on.

**The solution** 

I created several pieces of code to analysis online newspapers and social media. This particular code here scraps Twitter and returns structured information about the latest tweets.

**The risks** 

I couldn't scrap tweets that were older than 2 weeks
the keywords I used were basics and therefore, combined with the noise on the social platform, I could not trust 100% the results

**The tools**

I used several libraries:

1. [Aylien](aylien.com)
They are a company specialised in NLP product. They provide an API with a limit of calls per day. Their NLP library gave me a more precise results than the TwitterSearch Library as well as a percentage of confidence.

2. Google Sheet
I used the Google Sheet API because I am a big fan of observing my results directly in an Excel when I can to give me a clear results and also to share them quickly with others. This API set up has changed a little bit since I have written this code but the concept is the same. More information can be found [here](https://developers.google.com/sheets/api/)

3. TwitterSearch 
There is a lot of API wrapper for Twitte out there but I like this one in particular because of the clarity of the documentation and the functions provided. [This library](https://pypi.org/project/TwitterSearch/) has been created by the Technical University of Munich.

**The code**

*Importing the libraries*

<img src="images/Screen Shot 2019-08-08 at 8.45.53 pm.png?raw=true"/>

*Setting up the different API access*

The keys for Aylien can be found on their website 
The keys for Twitter Search can be generated by accessing the Twitter developer site 
The API access for Google Sheet might have slighlty changed recently 

<img src="images/Screen Shot 2019-08-08 at 8.46.48 pm.png?raw=true"/>

*Creating the main function* 

Creating the main function of the code, TwitterDf(t, keyword), which will use the limit number of retweet and a keyword as its arguments. 

notes: 

1. t: I have chosen to take the number of retweets as a proxy for the influence of this tweet. For some research I would only be interested in tweets that has a high-influence 
2. keyword: In some cases, I have used several keywords to narrow the research 

I start with opening a Google Sheet and start from the very first row 
<img src="images/Screen Shot 2019-08-08 at 8.55.22 pm.png?raw=true"/>

I then create an extra function to clean the text of the tweets and a serie of function to provide different metrics for the sentiment analysis
<img src="images/Screen Shot 2019-08-08 at 8.55.32 pm.png?raw=true"/>

Finally, I create the actual part that will scrap the tweets and organise them in dataframe within the Google Sheet. I have changed several times the fields I needed, dependending on my research. I found that having more background about the user would help me understand its influence and give more background. Then, regarding the tweet itself, I used a series of fields that are available to me and that can be more or less relevant. 
For example, while the retweet count is useful to understand the impact of the tweet, the location of the tweet was not always relevant. 

<img src="images/Screen Shot 2019-08-08 at 8.55.45 pm.png?raw=true"/>





